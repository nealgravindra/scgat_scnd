{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext memory_profiler\n",
    "\n",
    "import scanpy as sc\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import os,time,datetime,sys,pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc.settings.verbosity=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data obj loaded in 15.99-s @200226.20:53:57\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "pfp = '/home/ngr4/project/scnd/results/'\n",
    "pdfp = '/home/ngr4/project/scnd/data/processed/'\n",
    "\n",
    "if True :\n",
    "    start=time.time()\n",
    "    fname='fullnoimp.h5ad'\n",
    "    backed='r' # None if not, 'r+' if want to modify AnnData\n",
    "    adata = sc.read_h5ad(os.path.join(pdfp,fname),backed=backed)\n",
    "    print('Data obj loaded in {:.2f}-s @'.format(time.time()-start)+datetime.datetime.now().strftime('%y%m%d.%H:%M:%S'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early development prediction\n",
    "\n",
    "Induction task with all cell types\n",
    "\n",
    "#### TODO:\n",
    "- try with just PC\n",
    "- few show learning with PC (10 labels, e.g., 5wk_WT, 5wk_SCA1, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing neighbors\n",
      "WARNING: You’re trying to run this on 26374 dimensions of `.X`, if you really want this, set `use_rep='X'`.\n",
      "         Falling back to preprocessing with `sc.pp.pca` and default params.\n",
      "computing PCA with n_comps = 50\n",
      "    finished (0:01:10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/ysm/project/dijk/ngr4/conda_envs/py37dev/lib/python3.7/site-packages/numba/typed_passes.py:293: NumbaPerformanceWarning: \n",
      "The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n",
      "\n",
      "To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help.\n",
      "\n",
      "File \"../../conda_envs/py37dev/lib/python3.7/site-packages/umap/rp_tree.py\", line 135:\n",
      "@numba.njit(fastmath=True, nogil=True, parallel=True)\n",
      "def euclidean_random_projection_split(data, indices, rng_state):\n",
      "^\n",
      "\n",
      "  state.func_ir.loc))\n",
      "/gpfs/ysm/project/dijk/ngr4/conda_envs/py37dev/lib/python3.7/site-packages/umap/nndescent.py:92: NumbaPerformanceWarning: \n",
      "The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n",
      "\n",
      "To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help.\n",
      "\n",
      "File \"../../conda_envs/py37dev/lib/python3.7/site-packages/umap/utils.py\", line 409:\n",
      "@numba.njit(parallel=True)\n",
      "def build_candidates(current_graph, n_vertices, n_neighbors, max_candidates, rng_state):\n",
      "^\n",
      "\n",
      "  current_graph, n_vertices, n_neighbors, max_candidates, rng_state\n",
      "/gpfs/ysm/project/dijk/ngr4/conda_envs/py37dev/lib/python3.7/site-packages/numba/typed_passes.py:293: NumbaPerformanceWarning: \n",
      "The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n",
      "\n",
      "To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help.\n",
      "\n",
      "File \"../../conda_envs/py37dev/lib/python3.7/site-packages/umap/nndescent.py\", line 47:\n",
      "    @numba.njit(parallel=True)\n",
      "    def nn_descent(\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    finished (0:01:28)\n",
      "computing neighbors\n",
      "WARNING: You’re trying to run this on 26374 dimensions of `.X`, if you really want this, set `use_rep='X'`.\n",
      "         Falling back to preprocessing with `sc.pp.pca` and default params.\n",
      "computing PCA with n_comps = 50\n",
      "    finished (0:00:22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/ysm/project/dijk/ngr4/conda_envs/py37dev/lib/python3.7/site-packages/numba/typed_passes.py:293: NumbaPerformanceWarning: \n",
      "The keyword argument 'parallel=True' was specified but no transformation for parallel execution was possible.\n",
      "\n",
      "To find out why, try turning on parallel diagnostics, see http://numba.pydata.org/numba-doc/latest/user/parallel.html#diagnostics for help.\n",
      "\n",
      "File \"../../conda_envs/py37dev/lib/python3.7/site-packages/umap/nndescent.py\", line 47:\n",
      "    @numba.njit(parallel=True)\n",
      "    def nn_descent(\n",
      "    ^\n",
      "\n",
      "  state.func_ir.loc))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    finished (0:00:24)\n"
     ]
    }
   ],
   "source": [
    "# split data \n",
    "\n",
    "## sampling for induction, early devel pred\n",
    "### 5wk SCA1 [7294, 72931, 72932]\n",
    "### 5wk WT [7202, 72921, 72922]\n",
    "idx_train = np.arange(adata.shape[0])[(adata.obs['batch']!='7294') & (adata.obs['batch']!='7202')]\n",
    "idx_test = np.arange(adata.shape[0])[(adata.obs['batch']=='7294') | (adata.obs['batch']=='7202')]\n",
    "\n",
    "batchid_train = adata.obs['batch'][idx_train].to_list()\n",
    "batchid_test = adata.obs['batch'][idx_test].to_list()\n",
    "\n",
    "idx_train,_=train_test_split(idx_train,train_size=0.2,stratify=batchid_train)\n",
    "idx_test,_=train_test_split(idx_test,train_size=0.5,stratify=batchid_test)\n",
    "idx_train.sort()\n",
    "idx_test.sort()\n",
    "\n",
    "data_train = sc.AnnData(adata.X[idx_train.tolist()], obs=adata.obs.iloc[idx_train.tolist(),:]) # more mem tax, %memit sc.AnnData(sparse.csr_matrix(adata.X[idx_train.tolist()]))\n",
    "data_test = sc.AnnData(adata.X[idx_test.tolist()], obs=adata.obs.iloc[idx_test.tolist(),:]) \n",
    "\n",
    "if True :\n",
    "        scaler = MinMaxScaler()\n",
    "        data_train.X = scaler.fit_transform(data_train.X)\n",
    "        scaler = MinMaxScaler()\n",
    "        data_test.X = scaler.fit_transform(data_test.X)\n",
    "data_train.X = sparse.csr_matrix(data_train.X)\n",
    "data_test.X = sparse.csr_matrix(data_test.X)\n",
    "\n",
    "sc.pp.neighbors(data_train,n_neighbors=10,n_pcs=100)\n",
    "sc.pp.neighbors(data_test,n_neighbors=10,n_pcs=100)\n",
    "\n",
    "adj_train=data_train.uns['neighbors']['connectivities']+sparse.csr_matrix(np.eye(data_train.shape[0]))\n",
    "adj_test=data_test.uns['neighbors']['connectivities']+sparse.csr_matrix(np.eye(data_test.shape[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# io \n",
    "if True :\n",
    "    # export\n",
    "    train = {'labels':(data_train.obs['genotype']=='SCA1').astype(int).to_numpy(),\n",
    "             'adj':adj_train.todense(),\n",
    "             'node_features':data_train.X.todense()}\n",
    "    test = {'labels':(data_test.obs['genotype']=='SCA1').astype(int).to_numpy(),\n",
    "             'adj':adj_test.todense(),\n",
    "             'node_features':data_test.X.todense()}\n",
    "    with open(os.path.join(pdfp,'earlyDev_train.pickle'),'wb') as f:\n",
    "        pickle.dump(train, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(os.path.join(pdfp,'earlyDev_test.pickle'),'wb') as f:\n",
    "        pickle.dump(test, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "if False :\n",
    "    # import\n",
    "    with open(os.path.join(pdfp,'earlyDev_train_1percent.pickle'),'rb') as f:\n",
    "        dd = pickle.load(f) # test if b, np.sum(b['adj']==adj_train)==adj_train.shape[0]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict timepoint\n",
    "\n",
    "With just WT or pooled?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
