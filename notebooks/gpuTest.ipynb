{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Could not run 'aten::_fused_dropout' with arguments from the 'SparseCUDATensorId' backend. 'aten::_fused_dropout' is only available for these backends: [CUDATensorId, VariableTensorId].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-9c8fc96c0c94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[0mbest_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 481\u001b[0;31m     \u001b[0mloss_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'{}.pkl'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-9c8fc96c0c94>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m     \u001b[0mloss_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;31m#     loss_train = nn.BCEWithLogitsLoss(output[idx_train], labels[idx_train])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/ysm/project/dijk/ngr4/conda_envs/py37dev/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-9c8fc96c0c94>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, adj)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0matt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattentions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/ysm/project/dijk/ngr4/conda_envs/py37dev/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m    805\u001b[0m     return (_VF.dropout_(input, p, training)\n\u001b[1;32m    806\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m             else _VF.dropout(input, p, training))\n\u001b[0m\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Could not run 'aten::_fused_dropout' with arguments from the 'SparseCUDATensorId' backend. 'aten::_fused_dropout' is only available for these backends: [CUDATensorId, VariableTensorId]."
     ]
    }
   ],
   "source": [
    "'''\n",
    "`earlydev_induction.py`\n",
    "\n",
    "## Purpose\n",
    "----\n",
    "1. predict early development\n",
    "\n",
    "## Instructions\n",
    "----\n",
    "1. Create adj and data objs manually in nb w/bigger memory than gpu\n",
    "\n",
    "Modify script from: `ngr4@ruddle.hpc.yale.edu:/home/project/scni/scripts/induction_sample/is2/induction.py`\n",
    "\n",
    "Neal G. Ravindra, 24 Feb 2020\n",
    "\n",
    "TODO\n",
    "----\n",
    "*\n",
    "* clean up modules\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "# default for `matplotlib` font size\n",
    "plt.rc('font', size = 9)\n",
    "plt.rc('font', family='sans serif')\n",
    "plt.rcParams['legend.frameon']=False\n",
    "plt.rcParams['axes.grid']=False\n",
    "plt.rcParams['legend.markerscale']=0.5\n",
    "sc.set_figure_params(dpi=300,dpi_save=600,\n",
    "                     frameon=False,\n",
    "                     fontsize=9)\n",
    "plt.rcParams['savefig.dpi'] = 600\n",
    "sc.settings.verbosity=3\n",
    "\n",
    "\n",
    "# reproducibility\n",
    "rs = np.random.seed(42)\n",
    "\n",
    "\n",
    "# io\n",
    "pfp = '/home/ngr4/project/scnd/results/'\n",
    "pdfp = '/home/ngr4/project/scnd/data/processed/'\n",
    "sc.settings.figdir=pfp\n",
    "\n",
    "savefigs = True\n",
    "\n",
    "\n",
    "## utils.py\n",
    "## ----\n",
    "\n",
    "def normalize_adj(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
    "    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
    "    return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)\n",
    "\n",
    "\n",
    "def normalize_features(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n",
    "\n",
    "def scipysparse2torchsparse(x) :\n",
    "    '''\n",
    "    Input: scipy csr_matrix\n",
    "    Returns: torch tensor in experimental sparse format\n",
    "\n",
    "    REF: Code adatped from [PyTorch discussion forum](https://discuss.pytorch.org/t/better-way-to-forward-sparse-matrix/21915>)\n",
    "    '''\n",
    "    samples=x.shape[0]\n",
    "    features=x.shape[1]\n",
    "    values=x.data\n",
    "    coo_data=x.tocoo()\n",
    "    indices=torch.LongTensor([coo_data.row,coo_data.col]) # OR transpose list of index tuples\n",
    "    t=torch.sparse.FloatTensor(indices,torch.from_numpy(values).float(),[samples,features])\n",
    "    return t\n",
    "\n",
    "if True :\n",
    "    # load training data\n",
    "    with open(os.path.join(pdfp,'earlyDev_train_1percent.pickle'),'rb') as f:\n",
    "        data_train = pickle.load(f) # test if b, np.sum(b['adj']==adj_train)==adj_train.shape[0]**2\n",
    "\n",
    "    ## pre-process data 4torch\n",
    "    ## ----\n",
    "\n",
    "    # labels\n",
    "    labels = data_train['labels']\n",
    "    labels = torch.LongTensor(labels)\n",
    "\n",
    "    # adj\n",
    "    adj = data_train['adj']\n",
    "    adj = adj.to_dense()\n",
    "#     adj = scipysparse2torchsparse(adj)\n",
    "\n",
    "    # features\n",
    "    features = data_train['node_features']\n",
    "#     features = scipysparse2torchsparse(features)\n",
    "\n",
    "elif False :\n",
    "    # initial load\n",
    "    fname='fullnoimp.h5ad'\n",
    "    backed='r' # None if not, 'r+' if want to modify AnnData\n",
    "\n",
    "    start=time.time()\n",
    "    adata = sc.read_h5ad(os.path.join(pdfp,fname),backed=backed)\n",
    "    print('Data obj loaded in {:.2f}-s @'.format(time.time()-start)+datetime.datetime.now().strftime('%y%m%d.%H:%M:%S'))\n",
    "\n",
    "    # split data\n",
    "    ## sampling for induction, early devel pred\n",
    "    ### 5wk SCA1 [7294, 72931, 72932]\n",
    "    ### 5wk WT [7202, 72921, 72922]\n",
    "    idx_train = np.arange(adata.shape[0])[(adata.obs['batch']!='7294') & (adata.obs['batch']!='7202')]\n",
    "    idx_test = np.arange(adata.shape[0])[(adata.obs['batch']=='7294') | (adata.obs['batch']=='7202')]\n",
    "\n",
    "    batchid_train = adata.obs['batch'][idx_train].to_list()\n",
    "    batchid_test = adata.obs['batch'][idx_test].to_list()\n",
    "\n",
    "    idx_train,_=train_test_split(idx_train,train_size=0.01,stratify=batchid_train)\n",
    "    idx_test,_=train_test_split(idx_test,train_size=0.05,stratify=batchid_test)\n",
    "    idx_train.sort()\n",
    "    idx_test.sort()\n",
    "\n",
    "    data_train = sc.AnnData(adata.X[idx_train.tolist()], obs=adata.obs.iloc[idx_train.tolist(),:]) # more mem tax, %memit sc.AnnData(sparse.csr_matrix(adata.X[idx_train.tolist()]))\n",
    "    data_test = sc.AnnData(adata.X[idx_test.tolist()], obs=adata.obs.iloc[idx_test.tolist(),:])\n",
    "\n",
    "    data_train.X = sparse.csr_matrix(data_train.X)\n",
    "    data_test.X = sparse.csr_matrix(data_test.X)\n",
    "\n",
    "    sc.pp.neighbors(data_train,n_neighbors=10,n_pcs=100)\n",
    "    sc.pp.neighbors(data_test,n_neighbors=10,n_pcs=100)\n",
    "\n",
    "    adj_train=data_train.uns['neighbors']['connectivities']+sparse.csr_matrix(np.eye(data_train.shape[0]))\n",
    "    adj_test=data_test.uns['neighbors']['connectivities']+sparse.csr_matrix(np.eye(data_test.shape[0]))\n",
    "\n",
    "    if True :\n",
    "        # export objs\n",
    "        train = {'labels':(data_train.obs['genotype']=='SCA1').astype(int).to_numpy(),\n",
    "         'adj':adj_train,\n",
    "         'node_features':data_train.X}\n",
    "        test = {'labels':(data_test.obs['genotype']=='SCA1').astype(int).to_numpy(),\n",
    "                 'adj':adj_test,\n",
    "                 'node_features':data_test.X}\n",
    "        with open(os.path.join(pdfp,'earlyDev_train_1percent.pickle'),'wb') as f:\n",
    "            pickle.dump(train, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(os.path.join(pdfp,'earlyDev_test_1percent.pickle'),'wb') as f:\n",
    "            pickle.dump(test, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "else :\n",
    "    print('Print nothing was loaded\\n')\n",
    "\n",
    "\n",
    "## layers.py\n",
    "## ----\n",
    "\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        self.a = nn.Parameter(torch.zeros(size=(2*out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        h = torch.mm(input, self.W)\n",
    "        N = h.size()[0]\n",
    "\n",
    "        a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2 * self.out_features)\n",
    "        print('a_input',a_input.size())\n",
    "        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n",
    "        print('e', e.size())\n",
    "\n",
    "        zero_vec = -9e15*torch.ones_like(e)\n",
    "        attention = torch.where(adj > 0, e, zero_vec)\n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "#         self.attn = attention # NGR addition\n",
    "        h_prime = torch.matmul(attention, h)\n",
    "\n",
    "        if self.concat:\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
    "\n",
    "\n",
    "class SpecialSpmmFunction(torch.autograd.Function):\n",
    "    \"\"\"Special function for only sparse region backpropataion layer.\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, indices, values, shape, b):\n",
    "        assert indices.requires_grad == False\n",
    "        a = torch.sparse_coo_tensor(indices, values, shape)\n",
    "        ctx.save_for_backward(a, b)\n",
    "        ctx.N = shape[0]\n",
    "        return torch.matmul(a, b)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        a, b = ctx.saved_tensors\n",
    "        grad_values = grad_b = None\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_a_dense = grad_output.matmul(b.t())\n",
    "            edge_idx = a._indices()[0, :] * ctx.N + a._indices()[1, :]\n",
    "            grad_values = grad_a_dense.view(-1)[edge_idx]\n",
    "        if ctx.needs_input_grad[3]:\n",
    "            grad_b = a.t().matmul(grad_output)\n",
    "        return None, grad_values, None, grad_b\n",
    "\n",
    "\n",
    "class SpecialSpmm(nn.Module):\n",
    "    def forward(self, indices, values, shape, b):\n",
    "        return SpecialSpmmFunction.apply(indices, values, shape, b)\n",
    "\n",
    "\n",
    "class SpGraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Sparse version GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(SpGraphAttentionLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
    "        nn.init.xavier_normal_(self.W.data, gain=1.414)\n",
    "\n",
    "        self.a = nn.Parameter(torch.zeros(size=(1, 2*out_features)))\n",
    "        nn.init.xavier_normal_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "        self.special_spmm = SpecialSpmm()\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        dv = 'cuda' if input.is_cuda else 'cpu'\n",
    "\n",
    "        N = input.size()[0]\n",
    "        edge = adj.nonzero().t()\n",
    "\n",
    "        h = torch.mm(input, self.W)\n",
    "        # h: N x out\n",
    "#         assert not torch.isnan(h).any() # NGR edition\n",
    "        if torch.isnan(h).any() :\n",
    "            print('Hit h AssertionError')\n",
    "\n",
    "        # Self-attention on the nodes - Shared attention mechanism\n",
    "        edge_h = torch.cat((h[edge[0, :], :], h[edge[1, :], :]), dim=1).t()\n",
    "        # edge: 2*D x E\n",
    "\n",
    "        edge_e = torch.exp(self.leakyrelu(self.a.mm(edge_h).squeeze()))\n",
    "#         assert not torch.isnan(edge_e).any() # NGR edition\n",
    "        if torch.isnan(edge_e).any() :\n",
    "            print('Hit edge AssertionError')\n",
    "        # edge_e: E\n",
    "\n",
    "        e_rowsum = self.special_spmm(edge, edge_e, torch.Size([N, N]), torch.ones(size=(N,1), device=dv))\n",
    "        # e_rowsum: N x 1\n",
    "\n",
    "        edge_e = self.dropout(edge_e)\n",
    "        # edge_e: E\n",
    "\n",
    "        h_prime = self.special_spmm(edge, edge_e, torch.Size([N, N]), h)\n",
    "#         assert not torch.isnan(h_prime).any() # NGR edition\n",
    "        if torch.isnan(h_prime).any() :\n",
    "            print('Hit h_prime AssertionError')\n",
    "\n",
    "        # h_prime: N x out\n",
    "\n",
    "        h_prime = h_prime.div(e_rowsum)\n",
    "        # h_prime: N x out\n",
    "#         assert not torch.isnan(h_prime).any() # NGR edition\n",
    "        if torch.isnan(h_prime).any() :\n",
    "            print('Hit 2nd h_prime AssertionError')\n",
    "\n",
    "        if self.concat:\n",
    "            # if this layer is not last layer,\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            # if this layer is last layer,\n",
    "            return h_prime\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
    "\n",
    "\n",
    "# model.py\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):\n",
    "        \"\"\"Dense version of GAT.\"\"\"\n",
    "        super(GAT, self).__init__()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.attentions = [GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "\n",
    "        self.out_att = GraphAttentionLayer(nhid * nheads, nclass, dropout=dropout, alpha=alpha, concat=False)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.elu(self.out_att(x, adj))\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "class SpGAT(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):\n",
    "        \"\"\"Sparse version of GAT.\"\"\"\n",
    "        super(SpGAT, self).__init__()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.attentions = [SpGraphAttentionLayer(nfeat,\n",
    "                                                 nhid,\n",
    "                                                 dropout=dropout,\n",
    "                                                 alpha=alpha,\n",
    "                                                 concat=True) for _ in range(nheads)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "\n",
    "        self.out_att = SpGraphAttentionLayer(nhid * nheads,\n",
    "                                             nclass,\n",
    "                                             dropout=dropout,\n",
    "                                             alpha=alpha,\n",
    "                                             concat=False)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.elu(self.out_att(x, adj))\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "## train.py\n",
    "## ----\n",
    "\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False, help='Disables CUDA training.')\n",
    "parser.add_argument('--fastmode', action='store_true', default=True, help='Validate during training pass.')\n",
    "parser.add_argument('--sparse', action='store_true', default=True, help='GAT with sparse version or not.')\n",
    "parser.add_argument('--seed', type=int, default=72, help='Random seed.')\n",
    "parser.add_argument('--epochs', type=int, default=1000, help='Number of epochs to train.')\n",
    "parser.add_argument('--lr', type=float, default=0.001, help='Initial learning rate.')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4, help='Weight decay (L2 loss on parameters).')\n",
    "parser.add_argument('--hidden', type=int, default=8, help='Number of hidden units.')\n",
    "parser.add_argument('--nb_heads', type=int, default=8, help='Number of head attentions.')\n",
    "parser.add_argument('--dropout', type=float, default=0.6, help='Dropout rate (1 - keep probability).')\n",
    "parser.add_argument('--alpha', type=float, default=0.2, help='Alpha for the leaky_relu.')\n",
    "parser.add_argument('--patience', type=int, default=100, help='Patience')\n",
    "parser.add_argument('--clip', type=float, default=1, help='?')\n",
    "\n",
    "args = parser.parse_known_args()[0]\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# (old) Load data\n",
    "# adj, features, labels, idx_train, idx_val, idx_test = load_data()\n",
    "\n",
    "# Model and optimizer\n",
    "if args.sparse:\n",
    "    model = SpGAT(nfeat=features.shape[1],\n",
    "                nhid=args.hidden,\n",
    "                nclass=int(labels.max()) + 1,\n",
    "                dropout=args.dropout,\n",
    "                nheads=args.nb_heads,\n",
    "                alpha=args.alpha)\n",
    "else:\n",
    "    model = GAT(nfeat=features.shape[1],\n",
    "                nhid=args.hidden,\n",
    "                nclass=int(labels.max()) + 1,\n",
    "                dropout=args.dropout,\n",
    "                nheads=args.nb_heads,\n",
    "                alpha=args.alpha)\n",
    "# optimizer = optim.Adam(model.parameters(),\n",
    "#                        lr=args.lr,\n",
    "#                        weight_decay=args.weight_decay)\n",
    "optimizer = optim.Adagrad(model.parameters(),\n",
    "                          lr=args.lr,\n",
    "                          weight_decay=args.weight_decay)\n",
    "\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    adj = adj.cuda()\n",
    "    labels = labels.cuda()\n",
    "    # idx_train = idx_train.cuda()\n",
    "    # idx_val = idx_val.cuda()\n",
    "    # idx_test = idx_test.cuda()\n",
    "\n",
    "features, adj, labels = Variable(features), Variable(adj), Variable(labels)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = F.nll_loss(output, labels)\n",
    "#     loss_train = nn.BCEWithLogitsLoss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output, labels)\n",
    "    loss_train.backward()\n",
    "#     torch.nn.utils.clip_grad_norm_(model.parameters(),args.clip)\n",
    "    optimizer.step()\n",
    "\n",
    "    if not args.fastmode:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        model.eval()\n",
    "        output = model(features_val, adj_val)\n",
    "\n",
    "        loss_val = F.nll_loss(output, labels_val)\n",
    "    #     loss_val = nn.BCEWithLogitsLoss(output[idx_val], labels[idx_val])\n",
    "        acc_val = accuracy(output, labels_val)\n",
    "        print('Epoch: {:04d}'.format(epoch+1),\n",
    "              'loss_train: {:.4f}'.format(loss_train.data.item()),\n",
    "              'acc_train: {:.4f}'.format(acc_train.data.item()),\n",
    "              'loss_val: {:.4f}'.format(loss_val.data.item()),\n",
    "              'acc_val: {:.4f}'.format(acc_val.data.item()),\n",
    "              'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "        return loss_val.data.item()\n",
    "    else :\n",
    "        return loss_train.data.item()\n",
    "\n",
    "\n",
    "def compute_test():\n",
    "    model.eval()\n",
    "    output = model(features_test, adj_test)\n",
    "    loss_test = F.nll_loss(output, labels_test)\n",
    "#     loss_test = nn.BCEWithLogitsLoss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output, labels_test)\n",
    "    print(\"Test set results:\",\n",
    "          \"\\n    loss= {:.4f}\".format(loss_test.data.numpy()),\n",
    "          \"\\n    accuracy= {:.4f}\".format(acc_test.data.numpy()))\n",
    "\n",
    "# Train model\n",
    "t_total = time.time()\n",
    "loss_values = []\n",
    "bad_counter = 0\n",
    "best = args.epochs + 1\n",
    "best_epoch = 0\n",
    "for epoch in range(args.epochs):\n",
    "    loss_values.append(train(epoch))\n",
    "\n",
    "    torch.save(model.state_dict(), '{}.pkl'.format(epoch))\n",
    "    if loss_values[-1] < best:\n",
    "        best = loss_values[-1]\n",
    "        best_epoch = epoch\n",
    "        bad_counter = 0\n",
    "    else:\n",
    "        bad_counter += 1\n",
    "\n",
    "    if bad_counter == args.patience:\n",
    "        break\n",
    "\n",
    "    files = glob.glob('*.pkl')\n",
    "    for file in files:\n",
    "        epoch_nb = int(file.split('.')[0])\n",
    "        if epoch_nb < best_epoch:\n",
    "            os.remove(file)\n",
    "\n",
    "files = glob.glob('*.pkl')\n",
    "for file in files:\n",
    "    epoch_nb = int(file.split('.')[0])\n",
    "    if epoch_nb > best_epoch:\n",
    "        os.remove(file)\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# Restore best model\n",
    "print('Loading {}th epoch'.format(best_epoch))\n",
    "model.load_state_dict(torch.load('{}.pkl'.format(best_epoch)))\n",
    "\n",
    "# Testing\n",
    "compute_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[   0,    0,    0,  ..., 3002, 3002, 3002],\n",
       "                       [   0,    9,   13,  ..., 2986, 2994, 3002]]),\n",
       "       values=tensor([1.0000, 0.1418, 0.9957,  ..., 0.5447, 0.1609, 1.0000]),\n",
       "       size=(3003, 3003), nnz=42251, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjlayout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 2642.79 MiB, increment: 124.38 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit adj = data_train['adj'].todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 2305.45 MiB, increment: -66.56 MiB\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(pdfp,'earlyDev_train_1percent.pickle'),'rb') as f:\n",
    "        data_train = pickle.load(f) # test if b, np.sum(b['adj']==adj_train)==adj_train.shape[0]**2\n",
    "        \n",
    "%memit adj=data_train['adj']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
