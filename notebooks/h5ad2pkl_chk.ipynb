{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Grab data from AnnData object and use it to create a graph pkl\n",
    "\n",
    "## Background\n",
    "\n",
    "Data can be represented in a more complete way by utilizing edge features in graph attention networks and modifying the architecture based on empirical results from 2021 \n",
    "\n",
    "## Objective\n",
    "\n",
    "Use self-supervised learning to learn graphical representations of data and harness edge features in improving performance of predictive tasks\n",
    "\n",
    "## Methods\n",
    "\n",
    "GAT, edge features, self-supervised learning, representation learning, healthcare application, single-cell transcriptomic data\n",
    "\n",
    "- use batch labels from dataset, train GAT to get edge coefficients from preds of those labels, use this for \"batch effect correction\" in the model, either by penalizing reliance on these edge features, or controlling for them in the final model \n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext memory_profiler\n",
    "\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "import pickle\n",
    "import scanpy as sc\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "sc.settings.verbosity=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/ngr4/project/scnd/data/mouse_220805.h5ad',\n",
       " '/home/ngr4/project/scnd/data/mouse_210726.h5ad']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "dfp = '/home/ngr4/project/scnd/data'\n",
    "glob.glob(os.path.join(dfp, '*h5ad'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import random\n",
    "from scipy import sparse\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "\n",
    "from torch_geometric.data import Data, ClusterData, ClusterLoader\n",
    "\n",
    "\n",
    "# settings\n",
    "plt.rc('font', size = 9)\n",
    "plt.rc('font', family='sans serif')\n",
    "plt.rcParams['pdf.fonttype']=42\n",
    "plt.rcParams['ps.fonttype']=42\n",
    "plt.rcParams['text.usetex']=False\n",
    "plt.rcParams['legend.frameon']=False\n",
    "plt.rcParams['axes.grid']=False\n",
    "plt.rcParams['legend.markerscale']=0.5\n",
    "sc.set_figure_params(dpi=300,dpi_save=600,\n",
    "                     frameon=False,\n",
    "                     fontsize=9)\n",
    "plt.rcParams['savefig.dpi']=600\n",
    "sc.settings.verbosity=2\n",
    "sc._settings.ScanpyConfig.n_jobs=-1\n",
    "sns.set_style(\"ticks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "## check and modify\n",
    "\n",
    "From Leon:\n",
    "correct genotype\n",
    "\n",
    "·   5wk WT= 7202, 72921, 72922\n",
    "\n",
    "·   5wk SCA1= 7294, 72931, 72932\n",
    "\n",
    "·   12wk wild-type: 22018, 2061, 2062\n",
    "\n",
    "·   12wk SCA1: 22019, 2063, 2065\n",
    "\n",
    "·   18wk WT: 6569, 65701, 65702\n",
    "\n",
    "·   18wk SCA1: 6571, 65731, 65732\n",
    "\n",
    "·   24wk wild-type: 1974, 2020, 20202\n",
    "\n",
    "·   24wk SCA1: 1589, 2021, 20212\n",
    "\n",
    "·   30wk WT: 5812, #5822, 58232\n",
    "\n",
    "·   30wk SCA1: #58231, 58241, 58242\n",
    "\n",
    "\n",
    "58231 and 5822 have been assigned WT and SCA1 originally, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded in 54-s\n",
      "keys: dict_keys(['pg_data', 'metadata'])\n"
     ]
    }
   ],
   "source": [
    "pkl_out = '/home/ngr4/project/scnd/data/processed/mouse_220808_model_data.pkl'\n",
    "adata_out = '/home/ngr4/project/scnd/data/processed/mouse_220808.h5ad'\n",
    "adata_in = '/home/ngr4/project/scnd/data/mouse_220805.h5ad'\n",
    "\n",
    "slim_pkl = '/home/ngr4/project/scnd/data/processed/mouse_220808_model_data_slim.pkl'\n",
    "\n",
    "# load data\n",
    "tic = time.time()\n",
    "with open(slim_pkl, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    f.close()\n",
    "print('data loaded in {:.0f}-s'.format(time.time() - tic))\n",
    "print('keys:', data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify y label\n",
    "metadata = data['metadata']\n",
    "pg_data = data['pg_data']\n",
    "del data\n",
    "target_colname = 'y_genotype_crct'\n",
    "y_train = torch.tensor(metadata.loc[pg_data['train'].y, target_colname].to_numpy(dtype=np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # brief chk\n",
    "# # original\n",
    "# dt = data['metadata'].loc[:, ['batch', 'genotype_crct', 'timepoint']].drop_duplicates().sort_values(by=['timepoint', 'genotype_crct'])\n",
    "# dt.groupby(['timepoint', 'genotype_crct'])['batch'].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_data['train'].target = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## minibatching scheme\n",
    "def minibatcher(d, batch_size=None):\n",
    "    if batch_size is None:\n",
    "        batch_size = int((np.sqrt(d.x.shape[0]))/32)\n",
    "    cd = ClusterData(d, num_parts=int(np.sqrt(d.x.shape[0])))\n",
    "    return ClusterLoader(cd, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# initialize minibatcher\n",
    "    \n",
    "            \n",
    "    \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int((np.sqrt(pg_data['train'].x.shape[0]))/32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Data' object has no attribute 'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-32970228a05c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpg_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mpg_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpg_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpg_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Data' object has no attribute 'y'"
     ]
    }
   ],
   "source": [
    "train_idx = {i:k for i, k in enumerate(pg_data['train'].y)}\n",
    "del pg_data['train'].y\n",
    "pg_data['train'].idx = torch.arange(pg_data['train'].x.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing METIS partitioning...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "dataloader_train = minibatcher(pg_data['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.,  ..., 1., 0., 0.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Induction\n",
    "\n",
    "Sample 1/3 of the data randomly, grab labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample the data \n",
    "idx_train, idx_test = train_test_split(adata.obs.index, train_size=0.33)\n",
    "%memit tdata = sc.AnnData(X=adata[adata.obs.index.isin(idx_train),:].X, obs=adata[adata.obs.index.isin(idx_train),:].obs)\n",
    "temp = adata.obs.index[adata.obs.index.isin(idx_test)].to_list()\n",
    "idx_val, idx_test = train_test_split(temp, train_size=0.1)\n",
    "val = sc.AnnData(X=adata[adata.obs.index.isin(idx_val),:].X, obs=adata[adata.obs.index.isin(idx_val),:].obs)\n",
    "temp = adata.obs.index[adata.obs.index.isin(idx_test)].to_list()\n",
    "idx_test, _ = train_test_split(temp, train_size=0.11)\n",
    "test = sc.AnnData(X=adata[adata.obs.index.isin(idx_test),:].X, obs=adata[adata.obs.index.isin(idx_test),:].obs)\n",
    "\n",
    "def graph_pp(AnnData, bbknn=True):\n",
    "    sc.tl.pca(AnnData, n_comps=50)\n",
    "    if bbknn:\n",
    "        sc.external.pp.bbknn(AnnData)\n",
    "    else:\n",
    "        sc.pp.neighbors(AnnData, n_pcs=100, n_neighbors=30)\n",
    "    return AnnData\n",
    "\n",
    "# make graph\n",
    "tdata = graph_pp(tdata)\n",
    "val = graph_pp(val)\n",
    "test = graph_pp(test)\n",
    "\n",
    "if False:\n",
    "    del adata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding \n",
    "\n",
    "Select tasks for prediction\n",
    "\n",
    "1. yctype\n",
    "2. ysca1\n",
    "3. ygenotime (already done)\n",
    "4. SCA1_5/12/18/24/30wk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode ctype \n",
    "ctype_encoder = {v:i for i,v in enumerate(tdata.obs['ctype'].unique())}\n",
    "tdata.obs['yctype'] = tdata.obs['ctype'].map(ctype_encoder)\n",
    "val.obs['yctype'] = val.obs['ctype'].map(ctype_encoder)\n",
    "test.obs['yctype'] = test.obs['ctype'].map(ctype_encoder)\n",
    "\n",
    "# encode WT/SCA1 blended across time\n",
    "genotype_encoder = {'WT':0, 'SCA1':1}\n",
    "tdata.obs['ysca1'] = tdata.obs['genotype'].map(genotype_encoder).astype(int)\n",
    "val.obs['ysca1'] = val.obs['genotype'].map(genotype_encoder).astype(int)\n",
    "test.obs['ysca1'] = test.obs['genotype'].map(genotype_encoder).astype(int)\n",
    "\n",
    "# encode multi-label\n",
    "tdata.obs['genotype_timepoint'] = tdata.obs['genotype'].astype(str) + tdata.obs['timepoint'].astype(str).apply(lambda x: '_{}'.format(x))\n",
    "val.obs['genotype_timepoint'] = val.obs['genotype'].astype(str) + val.obs['timepoint'].astype(str).apply(lambda x: '_{}'.format(x))\n",
    "test.obs['genotype_timepoint'] = test.obs['genotype'].astype(str) + test.obs['timepoint'].astype(str).apply(lambda x: '_{}'.format(x))\n",
    "\n",
    "gt_encoder = {v:i for i,v in enumerate(tdata.obs['genotype_timepoint'].unique())}\n",
    "tdata.obs['ygenotime'] = tdata.obs['genotype_timepoint'].map(gt_encoder)\n",
    "val.obs['ygenotime'] = val.obs['genotype_timepoint'].map(gt_encoder)\n",
    "test.obs['ygenotime'] = test.obs['genotype_timepoint'].map(gt_encoder)\n",
    "\n",
    "# encode distinguishability of SCA1 at specific timepoints \n",
    "verbose = False\n",
    "tdata.obs['SCA1_5wk'] = (tdata.obs['genotype_timepoint']=='SCA1_5wk').astype(int)\n",
    "tdata.obs['SCA1_12wk'] = (tdata.obs['genotype_timepoint']=='SCA1_12wk').astype(int)\n",
    "tdata.obs['SCA1_18wk'] = (tdata.obs['genotype_timepoint']=='SCA1_18wk').astype(int)\n",
    "tdata.obs['SCA1_24wk'] = (tdata.obs['genotype_timepoint']=='SCA1_24wk').astype(int)\n",
    "tdata.obs['SCA1_30wk'] = (tdata.obs['genotype_timepoint']=='SCA1_30wk').astype(int)\n",
    "val.obs['SCA1_5wk'] = (val.obs['genotype_timepoint']=='SCA1_5wk').astype(int)\n",
    "val.obs['SCA1_12wk'] = (val.obs['genotype_timepoint']=='SCA1_12wk').astype(int)\n",
    "val.obs['SCA1_18wk'] = (val.obs['genotype_timepoint']=='SCA1_18wk').astype(int)\n",
    "val.obs['SCA1_24wk'] = (val.obs['genotype_timepoint']=='SCA1_24wk').astype(int)\n",
    "val.obs['SCA1_30wk'] = (val.obs['genotype_timepoint']=='SCA1_30wk').astype(int)\n",
    "test.obs['SCA1_5wk'] = (test.obs['genotype_timepoint']=='SCA1_5wk').astype(int)\n",
    "test.obs['SCA1_12wk'] = (test.obs['genotype_timepoint']=='SCA1_12wk').astype(int)\n",
    "test.obs['SCA1_18wk'] = (test.obs['genotype_timepoint']=='SCA1_18wk').astype(int)\n",
    "test.obs['SCA1_24wk'] = (test.obs['genotype_timepoint']=='SCA1_24wk').astype(int)\n",
    "test.obs['SCA1_30wk'] = (test.obs['genotype_timepoint']=='SCA1_30wk').astype(int)\n",
    "\n",
    "verbose = False\n",
    "if verbose:\n",
    "    # check encoding \n",
    "    print(tdata.obs['genotype_timepoint'].value_counts())\n",
    "    for i in ['SCA1_5wk', 'SCA1_12wk','SCA1_18wk','SCA1_24wk','SCA1_30wk']:\n",
    "        print(tdata.obs[i].sum())\n",
    "        \n",
    "    print(test.obs['genotype_timepoint'].value_counts())\n",
    "    for i in ['SCA1_5wk', 'SCA1_12wk','SCA1_18wk','SCA1_24wk','SCA1_30wk']:\n",
    "        print(test.obs[i].sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary\n",
    "def dictthat(AnnData, gene_ranger=True):\n",
    "    \"\"\"Prep dictionary for export.\n",
    "    \n",
    "    If gene_ranger, divide by zero can occur for \n",
    "    non-expressing genes. Thus, will floor those\n",
    "    to 0.\n",
    "    \n",
    "    NOTE: customization re:y to predict is highly\n",
    "    dependent on user input. ERGO, modify this \n",
    "    \n",
    "    Arguments:\n",
    "        AnnData (sc.AnnData): with graph stuff\n",
    "        \n",
    "    Returns:\n",
    "        dict\n",
    "    \"\"\"\n",
    "    if gene_ranger:\n",
    "        # each gene in [0,1], divide by zeros to 0\n",
    "        minimum = AnnData.X.min(axis=0)\n",
    "        maximum = AnnData.X.max(axis=0)\n",
    "        num = AnnData.X - minimum.todense()\n",
    "        denom =  (maximum - minimum).todense()\n",
    "        xhat = np.divide(num, denom, out=np.zeros_like(num), where=denom!=0) \n",
    "    else:\n",
    "        # matrix in [0,1]\n",
    "        xhat = (AnnData.X - AnnData.X.min()) / (AnnData.X.max() - AnnData.X.min())\n",
    "        \n",
    "    \n",
    "\n",
    "    gdata = {'X':xhat,\n",
    "             'adj':AnnData.uns['neighbors']['connectivities']+sparse.diags([1]*AnnData.shape[0], format='csr'),\n",
    "             'feature_names':AnnData.var_names.to_list()}\n",
    "    gdata['cell_id'] = AnnData.obs.index.to_list()\n",
    "    for col in AnnData.obs.columns:\n",
    "        gdata[col] = AnnData.obs[col].to_list()\n",
    "    \n",
    "    return gdata\n",
    "\n",
    "gdata_train = dictthat(tdata)\n",
    "gdata_val = dictthat(val)\n",
    "gdata_test  = dictthat(test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def pklthat(gdata, fname, fpath=pdfp): \n",
    "    with open(os.path.join(fpath,fname),'wb') as f :\n",
    "        pickle.dump(gdata, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        f.close()\n",
    "\n",
    "pklthat(gdata_train, 'scnd_train_200528.pkl')\n",
    "pklthat(gdata_val, 'scnd_val_200528.pkl')\n",
    "pklthat(gdata_test, 'scnd_test_200528.pkl')\n",
    "\n",
    "# clean\n",
    "if True:\n",
    "    del tdata, test, gdata_train, gdata_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modify pkl\n",
    "\n",
    "Add batch encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadpkl(filename):\n",
    "    with open (filename, 'rb') as f:\n",
    "        temp = pickle.load(f)\n",
    "        f.close()\n",
    "    return temp\n",
    "\n",
    "def add_batch(filename, date='200529'):\n",
    "    gdata = loadpkl(filename)\n",
    "    batch_encoder = {v:i for i,v in enumerate(np.unique(gdata['batch']))}\n",
    "    gdata['ybatch'] = list(map(batch_encoder.get, gdata['batch']))\n",
    "    pklthat(gdata, '{}_{}.pkl'.format(os.path.split(filename)[1].split('_20')[0], date))\n",
    "    del gdata\n",
    "    print('Batch added and pkl saved:\\n  {}_{}.pkl'.format(os.path.split(filename)[1].split('_20')[0], date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_batch(os.path.join(pdfp,'scnd_train_200528.pkl'))\n",
    "add_batch(os.path.join(pdfp,'scnd_val_200528.pkl'))\n",
    "add_batch(os.path.join(pdfp,'scnd_test_200528.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "gdata = loadpkl(os.path.join(pdfp,'scnd_train_200529.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
